{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cc13ee1-c8f3-4df4-a01a-bb308eef2272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "from torch import FloatTensor, LongTensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ale_py.roms import SpaceInvaders\n",
    "from ale_py import ALEInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff049773-a2e2-4ffe-afcf-f22adf83d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECAYRATE = 0.99\n",
    "GAMMA = 0.99\n",
    "LR = 0.0001\n",
    "BATCHSIZE = 5\n",
    "\n",
    "ale = ALEInterface()\n",
    "ale.loadROM(SpaceInvaders)\n",
    "env = gym.make('ALE/SpaceInvaders-v5', render_mode='human')\n",
    "state = env.reset()\n",
    "state_shape = state.shape\n",
    "\n",
    "def imcrop(im):\n",
    "    return im[20:-10]\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=48, kernel_size=3, stride=2),\n",
    "    nn.Conv2d(in_channels=48, out_channels=64, kernel_size=3, stride=2),\n",
    "    nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2880, 512),\n",
    "    nn.Linear(512, 96),\n",
    "    nn.Linear(96, 6),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e06ee-4bab-4b51-83a0-dc0fe5cc9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 76\n",
    "net.load_state_dict(torch.load(os.path.join(\"/Users/stack/Documents/warudo/polgradchk\", str(episode - 1) + \".pth\")))\n",
    "states, actions, rewards = [], [], []\n",
    "steps = 0\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=LR)\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    \n",
    "    # play out episode\n",
    "    while True:\n",
    "        probs = net(Variable(FloatTensor([imcrop(state)]).permute(0,3,1,2)))\n",
    "        action = Categorical(probs).sample()\n",
    "        action = action.data.numpy().astype(int)[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            reward = 0\n",
    "        states.append(Variable(FloatTensor([imcrop(state)]).permute(0,3,1,2)))\n",
    "        actions.append(float(action))\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break    \n",
    "    \n",
    "    # update policy\n",
    "    if episode % BATCHSIZE == 0:\n",
    "        print(\"updating policy\")\n",
    "        # strange implementation of discounted reward\n",
    "        running_add = 0\n",
    "        for i in reversed(range(steps)):\n",
    "            if rewards[i] == 0:\n",
    "                running_add = 0\n",
    "            else:\n",
    "                running_add = GAMMA * running_add + rewards[i]\n",
    "                rewards[i] = running_add\n",
    "        \n",
    "        # normalize reward\n",
    "        m, s = np.mean(rewards), np.std(rewards)\n",
    "        for i in range(steps):\n",
    "            rewards[i] = (rewards[i] - m) / s\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        for i in range(steps):\n",
    "            state = states[i]\n",
    "            action = Variable(FloatTensor([actions[i]]))\n",
    "            reward = rewards[i]\n",
    "            probs = net(state)\n",
    "            loss = -Categorical(probs).log_prob(action) * reward\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        torch.save(net.state_dict(), os.path.join(\"/Users/stack/Documents/warudo/polgradchk\", str(episode) + \".pth\"))\n",
    "        \n",
    "        states, actions, rewards = [], [], []\n",
    "        steps = 0\n",
    "    \n",
    "    print(\"episode \" + str(episode) + \" done\")\n",
    "    episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9c436-032c-4a08-804a-60dac1b34875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
