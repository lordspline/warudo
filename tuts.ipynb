{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd03255-2f93-4e06-8526-7f0cb249cc4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### simple cnn with cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367d2fd-fa9d-4d6f-ab42-d167bd00acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_imgs, train_labels), (test_imgs, test_labels) = datasets.cifar10.load_data()\n",
    "train_imgs, test_imgs = train_imgs / 255.0, test_imgs / 255.0\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "history = model.fit(train_imgs, train_labels, epochs=10, validation_data=(test_imgs, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0c3c1-79c5-453f-8e28-dadb985a377f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f18ab-a02e-4fe0-a692-f6db38ec691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=6, kernel_size=5, strides=1, activation=\"tanh\", input_shape=(28, 28, 1), padding=\"same\"))\n",
    "model.add(AveragePooling2D(pool_size=2, strides=2, padding=\"valid\"))\n",
    "model.add(Conv2D(filters=16, kernel_size=5, strides=1, activation=\"tanh\", input_shape=(28, 28, 1), padding=\"valid\"))\n",
    "model.add(AveragePooling2D(pool_size=2, strides=2, padding=\"valid\"))\n",
    "model.add(Conv2D(filters=120, kernel_size=5, strides=1, activation=\"tanh\", input_shape=(28, 28, 1), padding=\"valid\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=84, activation=\"tanh\"))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a8028-b4e0-4d1e-98c6-dabd53f1faa0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### simple gan for mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38648776-d495-48d4-860c-cf09dead4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_images, train_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(256)\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1,1), padding=\"same\", use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2,2), padding=\"same\", use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2,2), padding=\"same\", use_bias=False, activation=\"tanh\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding=\"same\", input_shape=[28,28,1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "cross_entropy = tf.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optim = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optim = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([256, 100])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optim.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
    "    discriminator_optim.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nat epoch \" + str(epoch))\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "        test_display = generator(tf.random.normal([2,100]), training=False)\n",
    "        plt.imsave(str(epoch) + \".jpg\", test_display[0,:,:,0] * 127.5 + 127.5, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3729d7-6a5c-4386-a289-88c06e46b4fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### simple style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8818e-0f19-4ec6-a63d-e01b1f3d3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import IPython.display as display\n",
    "\n",
    "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "style_path = tf.keras.utils.get_file('gogh.jpg','https://cdn.britannica.com/78/43678-050-F4DC8D93/Starry-Night-canvas-Vincent-van-Gogh-New-1889.jpg')\n",
    "\n",
    "def load_img(path):\n",
    "    maxdim = 512\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    \n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    scale = maxdim / max(shape)\n",
    "    \n",
    "    img = tf.image.resize(img, tf.cast(shape * scale, tf.int32))\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    return img\n",
    "\n",
    "content = load_img(content_path)\n",
    "style = load_img(style_path)\n",
    "\n",
    "def imshow(img):\n",
    "    if len(img.shape) > 3:\n",
    "        img = tf.squeeze(img)\n",
    "    plt.imshow(img)\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor * 255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor) > 3:\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "content_layers = [\n",
    "    'block5_conv2'\n",
    "]\n",
    "\n",
    "style_layers = [\n",
    "    'block1_conv1',\n",
    "    'block2_conv1',\n",
    "    'block3_conv1',\n",
    "    'block4_conv1',\n",
    "    'block5_conv1',\n",
    "]\n",
    "\n",
    "def vgg_layers(layers):\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "    outputs = [vgg.get_layer(layer).output for layer in layers]\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model\n",
    "\n",
    "def gram_matrix(inp):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', inp, inp)\n",
    "    inpshape = tf.shape(inp)\n",
    "    num_elems = tf.cast(inpshape[1] * inpshape[2], tf.float32)\n",
    "    return result / num_elems\n",
    "\n",
    "class StyleContentModel(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(StyleContentModel, self).__init__()\n",
    "        self.vgg = vgg_layers(style_layers + content_layers)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.vgg.trainable = False\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs = inputs * 255.0\n",
    "        preprocessed = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed)\n",
    "        \n",
    "        style_outputs, content_outputs = (outputs[:len(self.style_layers)], outputs[len(self.style_layers):])\n",
    "        style_outputs = [gram_matrix(output) for output in style_outputs]\n",
    "        \n",
    "        content_dict = {\n",
    "            content_name : value\n",
    "            for content_name, value in zip(self.content_layers, content_outputs)\n",
    "        }\n",
    "        \n",
    "        style_dict = {\n",
    "            style_name : value\n",
    "            for style_name, value in zip(self.style_layers, style_outputs)\n",
    "        }\n",
    "        \n",
    "        return {'content' : content_dict, 'style' : style_dict}\n",
    "\n",
    "extractor = StyleContentModel()\n",
    "\n",
    "style_targets = extractor(style)['style']\n",
    "content_targets = extractor(content)['content']\n",
    "\n",
    "def clipp(image):\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "\n",
    "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "style_weight = 1e-2\n",
    "content_weight = 1e4\n",
    "\n",
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    \n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name]) ** 2) for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / len(style_layers)\n",
    "    \n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_targets[name]) ** 2) for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / len(content_layers)\n",
    "    \n",
    "    return style_loss + content_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "    \n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clipp(image))\n",
    "\n",
    "image = tf.Variable(content)\n",
    "display.display(tensor_to_image(image))\n",
    "\n",
    "epochs = 20\n",
    "steps_per_epoch = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(steps_per_epoch):\n",
    "        train_step(image)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(tensor_to_image(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835695be-fd03-4f70-a1b4-22c0db2c143e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15b995-e4ea-43c0-ae43-1bfa27808580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8465da-da4e-42fa-a97e-225e8cd94627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
