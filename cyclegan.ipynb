{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336680ff-74e9-42eb-9931-8128c1555d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56ba9535-0f41-4954-b384-d0bb3edb1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('cycle_gan/horse2zebra', with_info=True, as_supervised=True)\n",
    "train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n",
    "test_horses, test_zebras = dataset['testA'], dataset['testB']\n",
    "\n",
    "BUFFERSIZE = 1000\n",
    "BATCHSIZE = 1\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "CHANNELS = 3\n",
    "LAMBDA = 10\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54df614f-7edc-4452-a8ea-ffa6addc1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    return tf.image.random_crop(image, size=[HEIGHT, WIDTH, 3])\n",
    "\n",
    "def normalize(image):\n",
    "    return (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "\n",
    "def random_jitter(image):\n",
    "    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    image = random_crop(image)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_train(image, label):\n",
    "    image = random_jitter(image)\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_test(image, label):\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "train_horses = train_horses.cache().map(preprocess_train, num_parallel_calls=tf.data.AUTOTUNE).shuffle(BUFFERSIZE).batch(BATCHSIZE)\n",
    "train_zebras = train_zebras.cache().map(preprocess_train, num_parallel_calls=tf.data.AUTOTUNE).shuffle(BUFFERSIZE).batch(BATCHSIZE)\n",
    "test_horses = test_horses.map(preprocess_test, num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(BUFFERSIZE).batch(BATCHSIZE)\n",
    "test_zebras = test_zebras.map(preprocess_test, num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(BUFFERSIZE).batch(BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fe80ee5-c28b-452f-a4d2-207560ac47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_g = pix2pix.unet_generator(CHANNELS, norm_type='instancenorm')\n",
    "gen_f = pix2pix.unet_generator(CHANNELS, norm_type='instancenorm')\n",
    "disc_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "disc_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89fe9692-38a0-4b92-899a-516043581cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def disc_loss(real, gen):\n",
    "    real_loss = loss_bce(tf.ones_like(real), real)\n",
    "    gen_loss = loss_bce(tf.zeros_like(gen), gen)\n",
    "    return 0.5 * (real_loss + gen_loss)\n",
    "def gen_loss(gen):\n",
    "    return loss_bce(tf.ones_like(gen), gen)\n",
    "def cycle_loss(real, cycled):\n",
    "    return LAMBDA * tf.reduce_mean(tf.abs(real - cycled))\n",
    "def identity_loss(real, same):\n",
    "    return 0.5 * LAMBDA * tf.reduce_mean(tf.abs(real - same))\n",
    "\n",
    "gen_g_optim = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "gen_f_optim = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "disc_x_optim = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "disc_y_optim = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab903d4b-768e-4ae2-abd3-386334990aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "    pred = model(test_input)\n",
    "    plt.figure()\n",
    "    disp_list = [test_input[0], pred[0]]\n",
    "    for i in range(2):\n",
    "        plt.subplot(1,2,i+1)\n",
    "        plt.imshow(disp_list[i] * 0.5 + 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f355d09-7ed3-490a-aa69-a838a60a94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_y = gen_g(real_x, training=True)\n",
    "        cycled_x = gen_f(fake_y, training=True)\n",
    "        \n",
    "        fake_x = gen_f(real_y, training=True)\n",
    "        cycled_y = gen_g(fake_x, training=True)\n",
    "        \n",
    "        same_x = gen_f(real_x, training=True)\n",
    "        same_y = gen_g(real_y, training=True)\n",
    "        \n",
    "        disc_real_x = disc_x(real_x, training=True)\n",
    "        disc_real_y = disc_y(real_y, training=True)\n",
    "        \n",
    "        disc_fake_x = disc_x(fake_x, training=True)\n",
    "        disc_fake_y = disc_y(fake_y, training=True)\n",
    "        \n",
    "        gen_g_loss = gen_loss(disc_fake_y)\n",
    "        gen_f_loss = gen_loss(disc_fake_x)\n",
    "        \n",
    "        total_cycle_loss = cycle_loss(real_x, cycled_x) + cycle_loss(real_y, cycled_y)\n",
    "        \n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "        \n",
    "        disc_x_loss = disc_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = disc_loss(disc_real_y, disc_fake_y)\n",
    "    \n",
    "    gen_g_grads = tape.gradient(total_gen_g_loss, gen_g.trainable_variables)\n",
    "    gen_f_grads = tape.gradient(total_gen_f_loss, gen_f.trainable_variables)\n",
    "    disc_x_grads = tape.gradient(disc_x_loss, disc_x.trainable_variables)\n",
    "    disc_y_grads = tape.gradient(disc_y_loss, disc_y.trainable_variables)\n",
    "    \n",
    "    gen_g_optim.apply_gradients(zip(gen_g_grads, gen_g.trainable_variables))\n",
    "    gen_f_optim.apply_gradients(zip(gen_f_grads, gen_f.trainable_variables))\n",
    "    disc_x_optim.apply_gradients(zip(disc_x_grads, disc_x.trainable_variables))\n",
    "    disc_y_optim.apply_gradients(zip(disc_y_grads, disc_y.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26c38f-62e6-4d02-8ff7-eba482dcafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_horse = next(iter(train_horses))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    n = 0\n",
    "    for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):\n",
    "        train_step(image_x, image_y)\n",
    "        if n % 10 == 0:\n",
    "            print('adsf')\n",
    "        n += 1\n",
    "    clear_output(wait=True)\n",
    "    generate_images(gen_g, sample_horse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
